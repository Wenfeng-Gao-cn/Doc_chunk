
deepseek-chat-DS_llm:
  model_name: "deepseek-chat"
  openai_api_key: "sk-1d437538f5404275b48425bb9e761613"
  openai_api_base: "https://api.deepseek.com"
  temperature: 0

deepseek-chat-siliconflow_llm:  #硅基流动
  model_name: "deepseek-ai/DeepSeek-V3" 
  openai_api_key: "sk-xucqkbqtmjyemzpumujgrtrfqmqxgapkgzmuqlrmhnyvgbhm"
  openai_api_base: "https://api.siliconflow.cn"
  temperature: 0


ernie-4.5-21b-a3b_llm:   # 百度云千帆——百度文心ERNIE-4.5-21B-A3B
  model_name: "ernie-4.5-21b-a3b"
  openai_api_key: "bce-v3/ALTAK-o6dxOMYkKAAgEYvkcn5pg/d1da7609a6dfee558cdbf43386f74e04cb30897a"
  openai_api_base: "https://qianfan.baidubce.com/v2/"
  temperature: 0

ERNIE-4.5-300B_llm:  #硅基流动
  model_name: "baidu/ERNIE-4.5-300B-A47B" 
  openai_api_key: "sk-xucqkbqtmjyemzpumujgrtrfqmqxgapkgzmuqlrmhnyvgbhm"
  openai_api_base: "https://api.siliconflow.cn"
  temperature: 0

Qwen30B_llm:  # 本地Qwen-30B模型  
  model_name: "/home/llm/models/pretrained/Qwen3-30B-A3B-FP8"
  openai_api_key: "sk-Feb1st"
  openai_api_base: "http://172.16.86.53:9981/v1/"
  temperature: 0

ernie-4.5-21b-a3b: # 百度云千帆——百度文心ERNIE-4.5-21B-A3B
  model_name: "ernie-4.5-21b-a3b"
  openai_api_key: "bce-v3/ALTAK-o6dxOMYkKAAgEYvkcn5pg/d1da7609a6dfee558cdbf43386f74e04cb30897a"
  openai_api_base: "https://qianfan.baidubce.com/v2/"
  temperature: 0

embedding_model: # 嵌入模型配置
  model_name: "Pro/BAAI/bge-m3"  # 支持OpenAI模型和第三方模型
  openai_api_key: "sk-xucqkbqtmjyemzpumujgrtrfqmqxgapkgzmuqlrmhnyvgbhm"
  openai_api_base: "https://api.siliconflow.cn"  
  max_retries: 3  # 最大重试次数
  request_timeout: 60  # 请求超时时间（秒）

Splitter_config: #文本分割器配置
  Recursive_config: #递归字符分割（按语义边界）
    chunk_size: 1000
    chunk_overlap: 200
    separators: ["\n\n", "\n", "。", "？", " ", ""]
  SentenceTransformers_config: #​按语义分割（嵌入模型驱动）
    #---------------通用设置--------------------#
    # breakpoint_threshold_type: "percentile"
    # breakpoint_threshold_amount: 0.8  # 仅切割显著跳跃点
    # sentence_split_regex: "(?<=[。？！])"
    # min_chunk_size: 50                  # 避免碎片化
    # buffer_size: 2 # 扩大上下文窗口
    #---------------学术文献场景-------------------#
    # breakpoint_threshold_type: "gradient"  # 策略类型
    # breakpoint_threshold_amount: 90.0       # 切割前10%的渐变峰值（默认95.0）
    # buffer_size: 10                         # 梯度计算窗口大小
    # add_start_index: True                     # 保留原始文本位置
    #---------------技术报告优化版（捕捉渐进主题）-------------------#
    # breakpoint_threshold_type: "gradient"
    # breakpoint_threshold_amount: 85.0        # 主题切换频繁时降低阈值
    #---------------技术文档优化版（降低敏感度）---------------------#
    breakpoint_threshold_type: "standard_deviation"
    breakpoint_threshold_amount: 2.0                # 专业术语易导致跳跃，需降低N值
    buffer_size: 5                                   # 扩大上下文窗口



# 向量数据库配置
vectordb_config:
  persist_directory: "./chroma_db"  # 向量数据库存储目录
  # collection_name: "xty_qa_collection"  # 集合名称
  collection_name: "general_test_collection"
  batch_size: 50  # 批量处理大小
  max_records: 0  # 最大处理记录数（0表示处理所有记录）

# 向量数据库的召回配置
retriever_config:
  # search_type: "similarity_score_threshold"
  search_type: "mmr"
  search_kwargs:
    k: 15 
    score_threshold: 0.75
    fetch_k: 50
    lambda_mult: 0.3

# QA匹配配置
graph_config:
  RAG_chat_llm: ["Qwen30B_llm","ernie-4.5-21b-a3b_llm","deepseek-chat-siliconflow_llm"]
  gen_metadata_llm: ["Qwen30B_llm","ernie-4.5-21b-a3b_llm","deepseek-chat-siliconflow_llm","deepseek-chat-DS_llm"] #生成切片的元数据。先使用第一个llm配置，如果出错，例如超出了模型的上下文就会按顺序使用第二个llm配置，直到最后一个llm配置

  chat_prompt: "prompt/chat_prompt.md"
  gen_metadata_prompt: "prompt/gen_metadata_prompt.md" #为切片生成metadata的提示词

  app_stream_mode: true  # 是否使用流式处理
  debug_logger: true


# 日志配置
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  log_file: "create_chunk.log"
  max_file_size: "10MB"
  backup_count: 5
